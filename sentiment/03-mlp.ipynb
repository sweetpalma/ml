{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea89a828-5952-4e48-9a25-a9887d61e093",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron\n",
    "\n",
    "In our [previous attempt](./02-logistic-regression-word2vec.ipynb), we tried using Word2Vec to improve our sentiment classification, but instead of a higher score, we got a much, much worse result. \n",
    "\n",
    "That happened because our existing architecture (logistic regression) was unfit for a new vectorization (seemingly much better) approach. But what if we change the architecture itself?\n",
    "\n",
    "That gives us a nice opportunity to try a different paradigm called **deep learning** - a branch of machine learning and artificial intelligence that uses artificial neural networks to process data and learn patterns. These networks, inspired by the structure of the human brain, are built with multiple layers of interconnected nodes (**neurons**) that allow them to identify complex relationships and make predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061541cc-7fc4-4164-bb8e-cfb52f0b8400",
   "metadata": {},
   "source": [
    "##  Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd252990-aeb4-4608-a977-b7b20ca08cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74676</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61691 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                               text\n",
       "0      Positive  I am coming to the borders and I will kill you...\n",
       "1      Positive  im getting on borderlands and i will kill you ...\n",
       "2      Positive  im coming on borderlands and i will murder you...\n",
       "3      Positive  im getting on borderlands 2 and i will murder ...\n",
       "4      Positive  im getting into borderlands and i can murder y...\n",
       "...         ...                                                ...\n",
       "74676  Positive  Just realized that the Windows partition of my...\n",
       "74677  Positive  Just realized that my Mac window partition is ...\n",
       "74678  Positive  Just realized the windows partition of my Mac ...\n",
       "74679  Positive  Just realized between the windows partition of...\n",
       "74680  Positive  Just like the windows partition of my Mac is l...\n",
       "\n",
       "[61691 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kagglehub\n",
    "df = kagglehub.dataset_load(\n",
    "    kagglehub.KaggleDatasetAdapter.PANDAS,\n",
    "    'jp797498e/twitter-entity-sentiment-analysis',\n",
    "    'twitter_training.csv',\n",
    "    pandas_kwargs={'encoding': 'ISO-8859-1'},\n",
    ")\n",
    "\n",
    "df = df[df.columns[[2, 3]]]\n",
    "df.columns = ['sentiment', 'text']\n",
    "\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['sentiment'] = df['sentiment'].astype(str)\n",
    "df = df.dropna()\n",
    "\n",
    "df = df.loc[df['sentiment'] != 'Irrelevant']\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe75828-602e-4478-8d47-6cfa6a76f93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "x_train = train['text']\n",
    "y_train = train['sentiment']\n",
    "x_test = test['text']\n",
    "y_test = test['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd019db4-9c0b-4987-a150-e297aa2e469b",
   "metadata": {},
   "source": [
    "## Semantic Vectorization\n",
    "\n",
    "Our vectorization routine also remains exactly the same. We are doing this so we can see how the change of *approach* affects the final result with the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b6e5a89-a45d-486c-9e5d-666c410d8329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\n",
    "    'leadbest/googlenewsvectorsnegative300', \n",
    "    path='GoogleNews-vectors-negative300.bin.gz'\n",
    ")\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "wv = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "\n",
    "import numpy as np\n",
    "from gensim.utils import simple_preprocess\n",
    "def vectorize(text):\n",
    "    tokens = simple_preprocess(text.lower(), deacc=True)\n",
    "    token_vectors = [wv.get_vector(x) for x in tokens if x in wv]\n",
    "    if token_vectors:\n",
    "        return np.mean(token_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(wv.vector_size)\n",
    "\n",
    "x_train = np.array(train['text'].map(vectorize).tolist())\n",
    "x_test = np.array(test['text'].map(vectorize).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6956af37-e711-4b85-b198-45fa047707a5",
   "metadata": {},
   "source": [
    "## Label Encoding\n",
    "\n",
    "Before we proceed further, we need to transform our output training as well! That happens because neural networks do not work with text *directly* - instead, we need to encode our labels, turning them into some kind of mathematical representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b10d854-699d-46a7-87ae-22469c6dfe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer \n",
    "import pandas as pd\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(df['sentiment'])\n",
    "\n",
    "y_train_encoded = pd.DataFrame(encoder.fit_transform(y_train))\n",
    "y_test_encoded = pd.DataFrame(encoder.transform(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9f3350-8b20-4d57-b92d-0bce1592eb48",
   "metadata": {},
   "source": [
    "This method is called \"one-hot encoding\", where each category is assigned a unique binary column. We have three categories - so our encodings will have a dimension of three each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcdb1495-d4e0-4702-bc6f-808b627ac924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49347</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49348</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49349</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49350</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49351</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49352 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1  2\n",
       "0      1  0  0\n",
       "1      1  0  0\n",
       "2      0  0  1\n",
       "3      0  1  0\n",
       "4      0  0  1\n",
       "...   .. .. ..\n",
       "49347  0  1  0\n",
       "49348  0  1  0\n",
       "49349  1  0  0\n",
       "49350  0  0  1\n",
       "49351  0  0  1\n",
       "\n",
       "[49352 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_train_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b1b3fe-5606-4c19-921a-612cf04099b2",
   "metadata": {},
   "source": [
    "## Building and Training the Model\n",
    "\n",
    "Now, let's design our model structure. This time, we will use a thing called **multilayer perceptron**. As its name states, it is a neural network that consists of multiple **layers** of neurons - allowing one to learn complex, non-linear relationships in data (unlike linear regression classifier).\n",
    "\n",
    "For this task, we will use three types of layers - input (transforms our source data and passes it next), dense (simple layer of interconnected neurons), and dropout (special layer that helps against overfitting by randomly disabling part of the previous layer during the training process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "131ab126-1711-41ef-82c2-44baa392baa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Sequential\n",
    "num_classes = len(encoder.classes_)\n",
    "model = Sequential([\n",
    "    layers.Input(shape=(wv.vector_size,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31137704-52a1-43f6-a6df-a3cd7462451f",
   "metadata": {},
   "source": [
    "Originally, there were only three layers - input, hidden, and output. But stacking two more hidden layers helped to uplift the original 82% accuracy to 85%. That happened due to increased model capacity, and (potential) hierarchical feature learning.\n",
    "\n",
    "We can compile and train our model now. Sticking to the CPU may be a good idea for now - using GPU for such a small model may actually slow down the training process due to a huge data transfer overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "469df364-c6d0-4a8f-90d7-90eeea7691cd",
   "metadata": {
    "scrolled": true,
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.5869 - loss: 0.9017 - val_accuracy: 0.6696 - val_loss: 0.7869\n",
      "Epoch 2/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.6685 - loss: 0.7735 - val_accuracy: 0.6872 - val_loss: 0.7375\n",
      "Epoch 3/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.6939 - loss: 0.7171 - val_accuracy: 0.7052 - val_loss: 0.7021\n",
      "Epoch 4/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7148 - loss: 0.6740 - val_accuracy: 0.7016 - val_loss: 0.7010\n",
      "Epoch 5/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7309 - loss: 0.6325 - val_accuracy: 0.7322 - val_loss: 0.6555\n",
      "Epoch 6/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7548 - loss: 0.5895 - val_accuracy: 0.7411 - val_loss: 0.6228\n",
      "Epoch 7/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7678 - loss: 0.5611 - val_accuracy: 0.7486 - val_loss: 0.5982\n",
      "Epoch 8/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7821 - loss: 0.5269 - val_accuracy: 0.7601 - val_loss: 0.5915\n",
      "Epoch 9/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7953 - loss: 0.4972 - val_accuracy: 0.7737 - val_loss: 0.5538\n",
      "Epoch 10/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8037 - loss: 0.4788 - val_accuracy: 0.7887 - val_loss: 0.5278\n",
      "Epoch 11/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8150 - loss: 0.4510 - val_accuracy: 0.7903 - val_loss: 0.5115\n",
      "Epoch 12/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8231 - loss: 0.4320 - val_accuracy: 0.7996 - val_loss: 0.4945\n",
      "Epoch 13/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8309 - loss: 0.4142 - val_accuracy: 0.7855 - val_loss: 0.5317\n",
      "Epoch 14/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8358 - loss: 0.4042 - val_accuracy: 0.8025 - val_loss: 0.4998\n",
      "Epoch 15/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8400 - loss: 0.3931 - val_accuracy: 0.7936 - val_loss: 0.5041\n",
      "Epoch 16/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8457 - loss: 0.3788 - val_accuracy: 0.8027 - val_loss: 0.4869\n",
      "Epoch 17/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8539 - loss: 0.3640 - val_accuracy: 0.8081 - val_loss: 0.4843\n",
      "Epoch 18/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8553 - loss: 0.3572 - val_accuracy: 0.8183 - val_loss: 0.4635\n",
      "Epoch 19/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8581 - loss: 0.3484 - val_accuracy: 0.8193 - val_loss: 0.4679\n",
      "Epoch 20/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8600 - loss: 0.3459 - val_accuracy: 0.8219 - val_loss: 0.4627\n",
      "Epoch 21/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8650 - loss: 0.3300 - val_accuracy: 0.8144 - val_loss: 0.4668\n",
      "Epoch 22/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8681 - loss: 0.3256 - val_accuracy: 0.8177 - val_loss: 0.4691\n",
      "Epoch 23/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8676 - loss: 0.3193 - val_accuracy: 0.8288 - val_loss: 0.4486\n",
      "Epoch 24/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8727 - loss: 0.3138 - val_accuracy: 0.8290 - val_loss: 0.4497\n",
      "Epoch 25/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8758 - loss: 0.3067 - val_accuracy: 0.8258 - val_loss: 0.4596\n",
      "Epoch 26/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8791 - loss: 0.2995 - val_accuracy: 0.8268 - val_loss: 0.4502\n",
      "Epoch 27/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8753 - loss: 0.3049 - val_accuracy: 0.8316 - val_loss: 0.4420\n",
      "Epoch 28/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8820 - loss: 0.2944 - val_accuracy: 0.8363 - val_loss: 0.4429\n",
      "Epoch 29/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8884 - loss: 0.2815 - val_accuracy: 0.8369 - val_loss: 0.4258\n",
      "Epoch 30/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8830 - loss: 0.2848 - val_accuracy: 0.8284 - val_loss: 0.4361\n",
      "Epoch 31/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8872 - loss: 0.2814 - val_accuracy: 0.8298 - val_loss: 0.4392\n",
      "Epoch 32/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8884 - loss: 0.2810 - val_accuracy: 0.8355 - val_loss: 0.4335\n",
      "Epoch 33/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8928 - loss: 0.2675 - val_accuracy: 0.8379 - val_loss: 0.4373\n",
      "Epoch 34/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8929 - loss: 0.2620 - val_accuracy: 0.8351 - val_loss: 0.4385\n",
      "Epoch 35/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8930 - loss: 0.2675 - val_accuracy: 0.8400 - val_loss: 0.4436\n",
      "Epoch 36/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8914 - loss: 0.2655 - val_accuracy: 0.8422 - val_loss: 0.4392\n",
      "Epoch 37/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8992 - loss: 0.2545 - val_accuracy: 0.8450 - val_loss: 0.4247\n",
      "Epoch 38/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9012 - loss: 0.2507 - val_accuracy: 0.8397 - val_loss: 0.4308\n",
      "Epoch 39/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8994 - loss: 0.2528 - val_accuracy: 0.8355 - val_loss: 0.4373\n",
      "Epoch 40/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8966 - loss: 0.2538 - val_accuracy: 0.8385 - val_loss: 0.4302\n",
      "Epoch 41/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8987 - loss: 0.2495 - val_accuracy: 0.8351 - val_loss: 0.4460\n",
      "Epoch 42/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9024 - loss: 0.2456 - val_accuracy: 0.8434 - val_loss: 0.4288\n",
      "Epoch 43/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9011 - loss: 0.2450 - val_accuracy: 0.8406 - val_loss: 0.4337\n",
      "Epoch 44/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9022 - loss: 0.2450 - val_accuracy: 0.8503 - val_loss: 0.4281\n",
      "Epoch 45/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9063 - loss: 0.2323 - val_accuracy: 0.8357 - val_loss: 0.4530\n",
      "Epoch 46/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9034 - loss: 0.2400 - val_accuracy: 0.8495 - val_loss: 0.4268\n",
      "Epoch 47/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9055 - loss: 0.2352 - val_accuracy: 0.8523 - val_loss: 0.4225\n",
      "Epoch 48/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9064 - loss: 0.2317 - val_accuracy: 0.8436 - val_loss: 0.4283\n",
      "Epoch 49/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9064 - loss: 0.2362 - val_accuracy: 0.8452 - val_loss: 0.4243\n",
      "Epoch 50/50\n",
      "\u001b[1m1388/1388\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9077 - loss: 0.2266 - val_accuracy: 0.8470 - val_loss: 0.4272\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import device\n",
    "with device('/CPU:0'):\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train_encoded, epochs=50, batch_size=32, validation_split=0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05325682-fc35-4c33-a305-d9b45158e874",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3deb7b76-c179-4f8b-9a4e-e27fe7fed9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.86      0.86      0.86      4471\n",
      "     Neutral       0.85      0.80      0.82      3666\n",
      "    Positive       0.83      0.87      0.85      4202\n",
      "\n",
      "    accuracy                           0.85     12339\n",
      "   macro avg       0.85      0.84      0.84     12339\n",
      "weighted avg       0.85      0.85      0.85     12339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "with device('/CPU:0'):\n",
    "    y_pred_probs = model.predict(x_test, verbose=False)\n",
    "    y_pred_labels = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true_labels = np.argmax(y_test_encoded.to_numpy(), axis=1)\n",
    "    print(classification_report(y_true_labels, y_pred_labels, target_names=encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b248b77f-1121-4d7e-a8fc-77fad0c3b698",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This experiment yielded an **85%** accuracy, confirming that a deeper, non-linear model can extract more signal from these embeddings than a linear classifier. The addition of hidden layers and dropout underscores the \"more capacity, better results\" principle, at least to a point. \n",
    "\n",
    "While a significant improvement over the 65% Logistic Regression baseline with these features, it still trails the 91% n-gram model. This suggests that to fully leverage Word2Vec semantic richness, architectures capable of processing sequences are the necessary next step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
