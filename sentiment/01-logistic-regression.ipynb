{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "310feb64-d6ab-4fc7-994e-172906344ad1",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "**Sentiment analysis** is the process of analyzing digital text to determine the emotional tone of the message - positive, negative, or neutral.  Essentially, it helps us understand the writer's attitude toward a particular topic or product, and it could be helpful for a lot of applications - like processing customer reviews or social media comments.\n",
    "\n",
    "This example demonstrates how to implement a simple sentiment classifier using logistic regression. It's surprising how well it performs for this class of tasks for a relatively simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb772fa-7a49-4892-99a8-5e6e0948f8d0",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Let's start with Standford's [IMGDB sentiment dataset](https://ai.stanford.edu/~amaas/data/sentiment/). Its official split is 50/50 - so we are going to have 25,000 samples for training and 25,0000 samples for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfa5bafd-dd47-496e-9e86-5efb5c3d04d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "train, test = load_dataset('stanfordnlp/imdb', split=['train', 'test'])\n",
    "class_names = train.features['label'].names\n",
    "\n",
    "x_train = np.array(train['text'])\n",
    "y_train = np.array(train['label'])\n",
    "x_test = np.array(test['text'])\n",
    "y_test = np.array(test['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3451e0b-4eb4-49f7-951a-25d48b41b039",
   "metadata": {},
   "source": [
    "Let's take a quick peek at our data before going further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf94f784-4ff6-415d-8a2b-f743f9c1dde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>A hit at the time but now better categorised a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>I love this movie like no other. Another time ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>This film and it's sequel Barry Mckenzie holds...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>'The Adventures Of Barry McKenzie' started lif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>The story centers around Barry McKenzie who mu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1      \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2      If only to avoid making this type of film in t...      0\n",
       "3      This film was probably inspired by Godard's Ma...      0\n",
       "4      Oh, brother...after hearing about this ridicul...      0\n",
       "...                                                  ...    ...\n",
       "24995  A hit at the time but now better categorised a...      1\n",
       "24996  I love this movie like no other. Another time ...      1\n",
       "24997  This film and it's sequel Barry Mckenzie holds...      1\n",
       "24998  'The Adventures Of Barry McKenzie' started lif...      1\n",
       "24999  The story centers around Barry McKenzie who mu...      1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425edca6-9479-4878-b34c-32faaf30bc18",
   "metadata": {},
   "source": [
    "## Building and Training the Model\n",
    "\n",
    "Now, that we're done with the data, it's time to build the classification pipeline. The first step would be **vectorization** - the process of turning strings to manipulate them mathematically. The simpliest version of it is called **сount vectorizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8640097-90a4-4903-a8b9-64fcf75095cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bye', 'hello', 'world'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 1],\n",
       "       [1, 0, 1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer_example = vectorizer.fit_transform(['Hello World! Hello!', 'Bye World']).toarray()\n",
    "display(vectorizer.get_feature_names_out(), vectorizer_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff0baf3-0e4f-486d-9740-9c9d1242e895",
   "metadata": {},
   "source": [
    "Its idea is pretty simple. First, it scans all the text to build a vocabulary of all the unique words it finds. Then, for each sentence, it creates a numerical list (vector) where each number describes how many times a specific word from that dictionary appears in that sentence.\n",
    "\n",
    "But in our specific case, we might take a look at another approach called **TF-IDF**. It measures a word's importance to a document by multiplying its frequency in that document (term frequency) by a penalty for how common the word is across all documents (inverse document frequency).\n",
    "\n",
    "This approach helps to essentially eliminate common terms from the classification process, emphasizing words that are uniquely relevant to the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9438a3e-158e-414c-afa0-0daf65559b21",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa51544-0831-4c43-b150-73d5a8c31770",
   "metadata": {},
   "source": [
    "Now, let's define a classifier (no fancy configuration here *yet*) and stick everything into an elegant pipeline. That is going to be our final **model architecture**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4687d9be-8a20-42fa-b0bb-d2ee4be3ed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc5131a-3090-4fc5-9439-b09d72563eb8",
   "metadata": {},
   "source": [
    "We could start training our model right away, but...\n",
    "\n",
    "It would be not ideal in terms of its *hyperparameters* - those values that define *how* our pipeline works. These are settings we choose before training, like regularization strength or how our text vectorizer processes words. They significantly control how the model learns and how well it ultimately performs.\n",
    "\n",
    "Manually trying every possible combination of these hyperparameters would be incredibly tedious. Instead, we can use automated hyperparameter tuning techniques. One such technique is **randomized search** - it randomly samples different combinations of hyperparameters from a pre-defined grid.\n",
    "\n",
    "We may tune the following parameters:\n",
    "\n",
    "- **Classifier `C`**: Regularization strength of the LogisticRegression classifier. Smaller values make it stronger (less prone to overfitting), and bigger - weaker (able to capture more nuances in noisy data).\n",
    "- **Vectorizer `ngram_range`**: This is crucial for capturing context! Instead of just looking at individual words (unigrams), n-grams allow us to consider sequences of words as single features. Using n-grams beyond unigrams often significantly improves performance in text tasks by providing more contextual information to the model, but it also increases the vocabulary size.\n",
    "- **Vectorizer `max_df`**: Maximum document frequency - ignore terms that appear in more than 'max_df' documents. Smaller values exclude more common terms (good for noise reduction), but too small may result in losing important common signals (underfitting).\n",
    "- **Vectorizer `min_df`**: Minimum document frequency - ignore terms that appear in fewer than 'min_df' documents. Smaller values may lead to huge noisy vocabularies, and bigger ones may result in losing specific signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db445460-7103-4a26-a64a-e2404f49a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2)], \n",
    "    'vectorizer__max_df': [0.85, 0.90, 0.95, 1.0],\n",
    "    'vectorizer__min_df': [1, 2, 3, 5],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfafe946-117a-494b-8bfd-74e15a6a1604",
   "metadata": {},
   "source": [
    "Everything is ready - let's train our model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11105c57-1e26-4f55-a16a-34fabf12785e",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 4/5] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.844 total time=  10.9s\n",
      "[CV 1/5] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.831 total time=  11.0s\n",
      "[CV 3/5] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1);, score=0.848 total time=  11.1s\n",
      "[CV 5/5] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.840 total time=  11.1s\n",
      "[CV 2/5] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.844 total time=  11.1s\n",
      "[CV 3/5] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.832 total time=  11.2s\n",
      "[CV 1/5] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1);, score=0.848 total time=  11.1s\n",
      "[CV 2/5] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1);, score=0.842 total time=  11.6s\n",
      "[CV 1/5] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.832 total time=   8.5s\n",
      "[CV 2/5] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.843 total time=   8.6s\n",
      "[CV 5/5] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.841 total time=   8.4s\n",
      "[CV 3/5] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.830 total time=   8.5s\n",
      "[CV 4/5] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.846 total time=   8.6s\n",
      "[CV 5/5] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1);, score=0.859 total time=   9.1s\n",
      "[CV 4/5] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1);, score=0.865 total time=   9.7s\n",
      "[CV 2/5] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1);, score=0.840 total time=   7.5s\n",
      "[CV 1/5] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1);, score=0.851 total time=   7.8s\n",
      "[CV 3/5] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1);, score=0.844 total time=   7.3s\n",
      "[CV 1/5] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.872 total time=  16.5s\n",
      "[CV 5/5] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1);, score=0.857 total time=   7.7s\n",
      "[CV 4/5] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1);, score=0.863 total time=   7.9s\n",
      "[CV 4/5] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.887 total time=  16.8s\n",
      "[CV 3/5] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.869 total time=  17.1s\n",
      "[CV 5/5] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.879 total time=  17.3s\n",
      "[CV 2/5] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.881 total time=  17.5s\n",
      "[CV 1/5] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1);, score=0.830 total time=   7.9s\n",
      "[CV 2/5] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1);, score=0.844 total time=   7.8s\n",
      "[CV 3/5] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1);, score=0.828 total time=   8.2s\n",
      "[CV 5/5] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1);, score=0.839 total time=   8.5s\n",
      "[CV 4/5] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1);, score=0.845 total time=   8.7s\n",
      "[CV 1/5] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.850 total time=   8.6s\n",
      "[CV 1/5] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2);, score=0.873 total time=  28.5s\n",
      "[CV 3/5] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.846 total time=   8.0s\n",
      "[CV 2/5] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.846 total time=   8.2s\n",
      "[CV 4/5] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.864 total time=   8.1s\n",
      "[CV 2/5] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2);, score=0.877 total time=  34.7s\n",
      "[CV 5/5] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.858 total time=   9.1s\n",
      "[CV 5/5] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2);, score=0.883 total time=  30.9s\n",
      "[CV 2/5] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.863 total time=   8.0s\n",
      "[CV 1/5] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.865 total time=   8.2s\n",
      "[CV 3/5] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.859 total time=   7.6s\n",
      "[CV 4/5] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.874 total time=   7.7s\n",
      "[CV 3/5] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2);, score=0.874 total time=  36.0s\n",
      "[CV 5/5] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.872 total time=   7.4s\n",
      "[CV 4/5] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2);, score=0.887 total time=  38.7s\n",
      "[CV 1/5] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.849 total time=   6.8s\n",
      "[CV 2/5] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.841 total time=   5.3s\n",
      "[CV 3/5] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.846 total time=   5.4s\n",
      "[CV 4/5] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.864 total time=   5.5s\n",
      "[CV 5/5] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.856 total time=   4.9s\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "from joblib import parallel_backend\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "cv = RandomizedSearchCV(pipeline, param_grid, random_state=0, n_jobs=-1, verbose=3)\n",
    "cv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861cf7c-2907-4ad4-ae8c-bc41d3899e5d",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86b04d1e-f56a-4e76-8b88-8f116e1ba5dd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.90      0.90      0.90     12500\n",
      "         pos       0.90      0.90      0.90     12500\n",
      "\n",
      "    accuracy                           0.90     25000\n",
      "   macro avg       0.90      0.90      0.90     25000\n",
      "weighted avg       0.90      0.90      0.90     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "prediction = cv.best_estimator_.predict(x_test)\n",
    "print(classification_report(y_test, prediction, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "286658e5-e369-49e2-9d09-2c2517ef50d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_b1923\">\n",
       "  <thead>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_b1923_row0_col0\" class=\"data row0 col0\" >vectorizer__ngram_range</td>\n",
       "      <td id=\"T_b1923_row0_col1\" class=\"data row0 col1\" >(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b1923_row1_col0\" class=\"data row1 col0\" >vectorizer__min_df</td>\n",
       "      <td id=\"T_b1923_row1_col1\" class=\"data row1 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b1923_row2_col0\" class=\"data row2 col0\" >vectorizer__max_df</td>\n",
       "      <td id=\"T_b1923_row2_col1\" class=\"data row2 col1\" >0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b1923_row3_col0\" class=\"data row3 col0\" >classifier__C</td>\n",
       "      <td id=\"T_b1923_row3_col1\" class=\"data row3 col1\" >10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x3a443a530>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "best_params_table = DataFrame.from_dict(cv.best_params_.items())\n",
    "display(best_params_table.style.hide(axis=0).hide(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf35528-810d-47ca-805a-73886014c47e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Our logistic regression model reached an accuracy of **90%**. \n",
    "\n",
    "This demonstrates the effectiveness of classical machine learning techniques for the type of text classification task. One of the key factors contributing to this performance was the use of n-grams for local context processing.\n",
    "\n",
    "While being highly effective, future improvements could involve exploring more complex vectorization techniques, experimenting with more advanced text pre-processing (like stemming or lemmatization), or even moving to deep learning models for sequence understanding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
