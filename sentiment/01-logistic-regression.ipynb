{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "310feb64-d6ab-4fc7-994e-172906344ad1",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Sentiment analysis, also known as opinion mining, is the process of computationally identifying and categorizing opinions expressed in a piece of text. Essentially, it helps us understand if the writer's attitude towards a particular topic, product, or entity is positive, negative, or neutral.\n",
    "\n",
    "This example demonstrates how to implement a simple sentiment classifier using logistic regression. It is surprising how well it performs for this class of tasks as a relatively simple model.\n",
    "\n",
    "Articles used:\n",
    "- https://www.getfocal.co/post/top-7-metrics-to-evaluate-sentiment-analysis-models\n",
    "- https://towardsdatascience.com/basics-of-countvectorizer-e26677900f9c/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb772fa-7a49-4892-99a8-5e6e0948f8d0",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Let's start from imporing some sentiment training dataset from Kaggle first. It is kind of weird, but it's enough to start *learning* something - we could clean it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfa5bafd-dd47-496e-9e86-5efb5c3d04d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONUNBUFFERED=1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2401</th>\n",
       "      <th>Borderlands</th>\n",
       "      <th>Positive</th>\n",
       "      <th>im getting on borderlands and i will murder you all ,</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74676</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74681 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       2401  Borderlands  Positive  \\\n",
       "0      2401  Borderlands  Positive   \n",
       "1      2401  Borderlands  Positive   \n",
       "2      2401  Borderlands  Positive   \n",
       "3      2401  Borderlands  Positive   \n",
       "4      2401  Borderlands  Positive   \n",
       "...     ...          ...       ...   \n",
       "74676  9200       Nvidia  Positive   \n",
       "74677  9200       Nvidia  Positive   \n",
       "74678  9200       Nvidia  Positive   \n",
       "74679  9200       Nvidia  Positive   \n",
       "74680  9200       Nvidia  Positive   \n",
       "\n",
       "      im getting on borderlands and i will murder you all ,  \n",
       "0      I am coming to the borders and I will kill you...     \n",
       "1      im getting on borderlands and i will kill you ...     \n",
       "2      im coming on borderlands and i will murder you...     \n",
       "3      im getting on borderlands 2 and i will murder ...     \n",
       "4      im getting into borderlands and i can murder y...     \n",
       "...                                                  ...     \n",
       "74676  Just realized that the Windows partition of my...     \n",
       "74677  Just realized that my Mac window partition is ...     \n",
       "74678  Just realized the windows partition of my Mac ...     \n",
       "74679  Just realized between the windows partition of...     \n",
       "74680  Just like the windows partition of my Mac is l...     \n",
       "\n",
       "[74681 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%env PYTHONUNBUFFERED=1\n",
    "import kagglehub\n",
    "df = kagglehub.dataset_load(\n",
    "    kagglehub.KaggleDatasetAdapter.PANDAS,\n",
    "    'jp797498e/twitter-entity-sentiment-analysis',\n",
    "    'twitter_training.csv',\n",
    "    pandas_kwargs={'encoding': 'ISO-8859-1'},\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a221ed-3ee7-4239-a2b7-c4d0853db666",
   "metadata": {},
   "source": [
    "Now, we can prepare a simple cleaning function. It's pretty simple - it just converts text into lowercase and removes common words (stopwords) that usually have no semantic value in terms of sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67a36e71-d304-44e1-8a79-16ecf8a4625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "def clean(text):\n",
    "    return remove_stopwords(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce3c426-a2bb-422d-880f-4db2bea5e349",
   "metadata": {},
   "source": [
    "Let's clean our dataset and take a look at it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "320fc0f9-e731-4ff6-9c6b-1564eeed2f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>coming borders kill all,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting borderlands kill all,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming borderlands murder all,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting borderlands 2 murder all,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting borderlands murder all,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74676</th>\n",
       "      <td>Positive</td>\n",
       "      <td>realized windows partition mac like 6 years nv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>Positive</td>\n",
       "      <td>realized mac window partition 6 years nvidia d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>Positive</td>\n",
       "      <td>realized windows partition mac 6 years nvidia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>Positive</td>\n",
       "      <td>realized windows partition mac like 6 years nv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>Positive</td>\n",
       "      <td>like windows partition mac like 6 years driver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61691 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                               text\n",
       "0      Positive                           coming borders kill all,\n",
       "1      Positive                   im getting borderlands kill all,\n",
       "2      Positive                  im coming borderlands murder all,\n",
       "3      Positive               im getting borderlands 2 murder all,\n",
       "4      Positive                 im getting borderlands murder all,\n",
       "...         ...                                                ...\n",
       "74676  Positive  realized windows partition mac like 6 years nv...\n",
       "74677  Positive  realized mac window partition 6 years nvidia d...\n",
       "74678  Positive  realized windows partition mac 6 years nvidia ...\n",
       "74679  Positive  realized windows partition mac like 6 years nv...\n",
       "74680  Positive  like windows partition mac like 6 years driver...\n",
       "\n",
       "[61691 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pin column names\n",
    "df = df[df.columns[[2, 3]]]\n",
    "df.columns = ['sentiment', 'text']\n",
    "\n",
    "# Pin column data types\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['sentiment'] = df['sentiment'].astype(str)\n",
    "df = df.dropna()\n",
    "\n",
    "# Clean data\n",
    "df = df.loc[df['sentiment'] != 'Irrelevant']\n",
    "df['text'] = df['text'].map(clean)\n",
    "\n",
    "# Check data\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e60ce2e9-9a67-48f0-8e18-7722a55abea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "x_train = train['text']\n",
    "y_train = train['sentiment']\n",
    "x_test = test['text']\n",
    "y_test = test['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425edca6-9479-4878-b34c-32faaf30bc18",
   "metadata": {},
   "source": [
    "## Building and Training the Model\n",
    "\n",
    "Alright, we are done with our data, now let's build the classification pipeline.\n",
    "\n",
    "We need to vectorize our data first - essentially, to turn strings into a bunch of numbers to manipulate them mathematically. The easiest way to do so is to use an instrument called **count vectorizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9438a3e-158e-414c-afa0-0daf65559b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdbda53-4f6a-4e38-8d32-90ea8904343e",
   "metadata": {},
   "source": [
    "First, it scans all the text to build a dictionary (vocabulary) of all unique words it finds. Then, for each sentence, it creates a numerical list (vector) where each number represents how many times a specific word from that dictionary appears in that sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8640097-90a4-4903-a8b9-64fcf75095cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bye', 'hello', 'world'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 1],\n",
       "       [1, 0, 1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_vector = vectorizer.fit_transform(['Hello World! Hello!', 'Bye World']).toarray()\n",
    "display(vectorizer.get_feature_names_out(), example_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe7f9c8-9bae-4dbc-812f-9427c347aa27",
   "metadata": {},
   "source": [
    "Sounds easy, right? Next, we will define a thing called **scaler**.\n",
    "\n",
    "It helps to rescale numerical features (like word counts) so they have a similar range, typically with a mean of 0 and a standard deviation of 1. This helps Logistic Regression learn better because it prevents features with naturally larger values from unfairly dominating the learning process, ensuring all features contribute more equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f88c46dd-565a-49bc-9468-c14c35d9e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_mean=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa51544-0831-4c43-b150-73d5a8c31770",
   "metadata": {},
   "source": [
    "Finally, let's define a classifier (no fancy configuration here *yet*) and stuck everything into an elegant pipeline. That would be our final **model architecture**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4687d9be-8a20-42fa-b0bb-d2ee4be3ed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(solver='lbfgs', max_iter=1500)\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('scaler', scaler),\n",
    "    ('classifier', classifier),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc5131a-3090-4fc5-9439-b09d72563eb8",
   "metadata": {},
   "source": [
    "We could start training our model right away, but...\n",
    "\n",
    "It would be not ideal in terms of its *hyperparameters* - those values that define *how* our pipeline works. These are settings we choose before training, like regularization strength (`C`) or how our text vectorizer processes words (`min_df`, `max_df`). They significantly control how the model learns and how well it ultimately performs.\n",
    "\n",
    "Manually trying every possible combination of these hyperparameters would be incredibly tedious. Instead, we can use automated hyperparameter tuning techniques. For example, `GridSearchCV` (or `RandomizedSearchCV`) systematically tries out different combinations of hyperparameters from a grid we define.\n",
    "\n",
    "Crucially, to evaluate how good each combination is without peeking at our final test set, it uses cross-validation. This means for each hyperparameter set, it splits the training data into several \"folds\", trains the model on some folds, and tests it on a remaining fold, repeating this process so every fold gets to be a test set. By averaging the performance across these folds, we get a more reliable score for that hyperparameter combination, helping us pick the best ones for our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db445460-7103-4a26-a64a-e2404f49a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2)], \n",
    "    'vectorizer__max_df': [0.85, 0.90, 0.95, 1.0],\n",
    "    'vectorizer__min_df': [1, 2, 3, 5],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfafe946-117a-494b-8bfd-74e15a6a1604",
   "metadata": {},
   "source": [
    "Let's dissect those hyperparameters:\n",
    "\n",
    "- Classifier `C`: Regularization strength of the LogisticRegression classifier. Smaller values make it stronger (less prone to overfitting), bigger - weaker (able to capture more nuances in data).\n",
    "- Vectorizer `ngram_range`: This is crucial for capturing context! Instead of just looking at individual words (unigrams), n-grams allow us to consider sequences of words as single features. Using n-grams beyond unigrams often significantly improves performance in text tasks by providing more contextual information to the model, but also increases the vocabulary size.\n",
    "- Vectorizer `max_df`: Maximum document frequency - ignore terms that appear in more than 'max_df' % of documents. Smaller values exclude more common terms (good for noise reduction), but too small may result in losing important common signals (underfitting).\n",
    "- Vectorizer `min_df`: Minimum document frequency - ignore terms that appear in fewer than 'min_df' documents. Smaller values may lead to huge noisy vocabularies, bigger may result in losing specific signals.\n",
    "\n",
    "Here comes the moment of truth - we could finally start actually *training* our model, with cross-validation running over the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11105c57-1e26-4f55-a16a-34fabf12785e",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONUNBUFFERED=1\n",
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   1.7s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   1.8s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   1.9s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   3.7s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   3.8s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   4.2s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   3.3s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   3.5s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   3.4s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  14.0s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  15.7s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  13.4s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  19.9s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   2.8s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   2.7s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   3.0s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  11.2s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  21.6s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  24.5s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  11.4s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   4.0s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   4.1s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  10.3s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=   5.9s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   3.4s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=   6.4s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=   6.3s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   3.6s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   3.7s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   3.7s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   3.7s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   4.0s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   3.4s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  13.7s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  15.4s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  13.9s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   2.6s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  21.3s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  10.6s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   2.2s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  20.8s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   2.0s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=   9.9s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  23.7s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   3.1s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=   8.7s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=   4.9s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   3.0s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=   5.2s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   3.0s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=   5.2s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   3.1s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   3.3s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   3.6s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   3.3s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   3.6s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   3.5s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  13.5s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  15.1s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  13.9s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  10.4s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   2.5s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   2.3s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  20.9s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   2.1s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  21.0s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  10.8s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  24.4s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=   5.6s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   3.9s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=   9.2s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   3.7s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   3.5s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=   5.7s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=   6.6s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   3.3s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   3.5s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   3.7s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   3.9s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   4.7s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   3.7s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  14.6s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  15.8s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  14.4s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   2.5s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  10.8s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  21.2s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  22.0s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   2.2s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  10.7s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  24.2s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=   5.1s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=   5.3s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=   8.8s\n",
      "[CV] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=   5.5s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   7.3s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   7.6s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   7.3s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   7.7s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   7.4s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   6.8s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   6.9s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   7.9s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   7.5s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  35.2s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  42.0s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  23.9s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  36.4s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  41.5s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   4.5s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  24.0s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   5.5s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   5.2s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  53.6s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  55.7s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   7.7s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   8.2s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  13.3s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  14.4s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  21.7s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.85, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  13.9s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   8.6s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   8.1s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   7.2s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   8.3s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   7.1s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   7.3s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   6.9s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  34.8s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  22.5s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  42.4s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  40.8s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   4.7s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  35.5s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  23.7s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   6.2s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   5.9s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  57.0s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  59.0s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=  10.2s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  15.0s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   9.9s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  15.8s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  23.0s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  15.0s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   9.1s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   7.7s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   9.1s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   9.3s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   6.8s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   7.6s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   7.8s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  38.2s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  25.3s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  44.6s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  37.2s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  43.1s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   4.6s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  24.4s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   4.8s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   5.6s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  58.3s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  55.4s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   7.1s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   7.4s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  11.9s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  11.9s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  19.1s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   6.9s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  11.8s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   7.6s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   7.5s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=   7.0s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   7.5s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   7.9s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   7.4s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  36.5s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  24.2s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  42.8s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  41.7s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  37.2s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   5.2s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  24.7s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   5.9s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   5.0s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  54.6s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  54.1s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   7.7s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  10.8s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  12.3s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   8.0s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   7.5s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  20.2s\n",
      "[CV] END classifier__C=1, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  11.5s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=  11.0s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=  12.9s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=  18.0s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  19.6s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  29.2s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   9.2s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=  15.4s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=  10.8s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  33.6s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  33.3s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  23.5s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  32.3s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  10.9s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   6.8s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   6.6s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   7.1s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  14.0s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   8.9s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   8.3s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   9.1s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  17.5s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  16.1s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  26.4s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  23.4s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=  11.0s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=  13.2s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  30.0s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=  19.0s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  20.6s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   8.4s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  32.5s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=  14.8s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  32.7s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   9.8s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  23.8s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  10.0s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   6.8s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   7.4s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   7.3s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  30.9s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  12.9s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   8.3s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   8.0s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   9.6s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  15.7s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  17.6s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  25.5s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  23.6s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=  10.6s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=  13.9s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=  18.8s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  30.3s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  21.1s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  33.8s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  34.2s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   8.7s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=  14.6s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=  11.0s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  25.4s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  11.5s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   7.8s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   7.9s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  33.1s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   7.1s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  14.4s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   8.7s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   8.6s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1); total time=   9.7s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  17.6s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  18.3s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  27.3s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  24.9s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=  10.9s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=  13.3s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1); total time=  18.6s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  29.8s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  20.2s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  32.9s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2); total time=  32.6s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   8.2s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=  14.1s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1); total time=   9.5s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  23.9s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  10.3s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 2); total time=  31.4s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   7.6s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   7.5s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1); total time=   7.8s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  12.6s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  10.5s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  12.6s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2); total time=  17.0s\n",
      "[CV] END classifier__C=10, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__ngram_range=(1, 2); total time=  19.7s\n"
     ]
    }
   ],
   "source": [
    "%env PYTHONUNBUFFERED=1\n",
    "from joblib import parallel_backend\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1, verbose=2)\n",
    "with parallel_backend('multiprocessing'):\n",
    "    grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861cf7c-2907-4ad4-ae8c-bc41d3899e5d",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86b04d1e-f56a-4e76-8b88-8f116e1ba5dd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.93      0.91      0.92      4453\n",
      "     Neutral       0.92      0.89      0.91      3701\n",
      "    Positive       0.88      0.93      0.90      4185\n",
      "\n",
      "    accuracy                           0.91     12339\n",
      "   macro avg       0.91      0.91      0.91     12339\n",
      "weighted avg       0.91      0.91      0.91     12339\n",
      "\n",
      "Best parameters: {'classifier__C': 0.1, 'vectorizer__max_df': 0.85, 'vectorizer__min_df': 1, 'vectorizer__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "prediction = grid.best_estimator_.predict(x_test)\n",
    "print(classification_report(y_test, prediction))\n",
    "print(f'Best parameters: {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf35528-810d-47ca-805a-73886014c47e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "By systematically preprocessing the data, constructing a robust Scikit pipeline (`CountVectorizer` -> `StandardScaler` -> `LogisticRegression`), and performing hyperparameter optimization via `GridSearchCV`, this sentiment analysis model achieved a final accuracy of 91%.\n",
    "\n",
    "Key factors contributing to this performance included the use of n-grams (1, 2) and careful tuning of regularization and vectorizer frequency cutoffs. This demonstrates the effectiveness of classical machine learning techniques for type of text classification task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
