{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "310feb64-d6ab-4fc7-994e-172906344ad1",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "**Sentiment analysis** is the process of analyzing digital text to determine the emotional tone of the message - positive, negative, or neutral.  Essentially, it helps us understand the writer's attitude toward a particular topic or product, and it could be helpful for a lot of applications - like processing customer reviews or social media comments.\n",
    "\n",
    "This example demonstrates how to implement a simple sentiment classifier using logistic regression. It's surprising how well it performs for this class of tasks for a relatively simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb772fa-7a49-4892-99a8-5e6e0948f8d0",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Let's start with Standford's [IMGDB sentiment dataset](https://ai.stanford.edu/~amaas/data/sentiment/). It contains 50,000 movie reviews, tagged either as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfa5bafd-dd47-496e-9e86-5efb5c3d04d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After the SuperFriends and Scooby Doo left the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good job.that's how i would describe this anim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michael Cacoyannis has had a relatively long c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've just seen this film in a lovely air-condi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My one-line summary hints that this is not a g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>***SPOILERS*** ***SPOILERS*** After two so-so ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>Way back in 1967, a certain director had no id...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>I saw this movie with my dad. I must have been...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>During my teens or should I say prime time I w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>Paranoid Park is about Alex, a 16 year old ska...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      After the SuperFriends and Scooby Doo left the...      1\n",
       "1      good job.that's how i would describe this anim...      1\n",
       "2      Michael Cacoyannis has had a relatively long c...      1\n",
       "3      I've just seen this film in a lovely air-condi...      0\n",
       "4      My one-line summary hints that this is not a g...      1\n",
       "...                                                  ...    ...\n",
       "39995  ***SPOILERS*** ***SPOILERS*** After two so-so ...      1\n",
       "39996  Way back in 1967, a certain director had no id...      0\n",
       "39997  I saw this movie with my dad. I must have been...      1\n",
       "39998  During my teens or should I say prime time I w...      1\n",
       "39999  Paranoid Park is about Alex, a 16 year old ska...      0\n",
       "\n",
       "[40000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('stanfordnlp/imdb', split='train+test')\n",
    "train, test = ds.train_test_split(test_size=0.2, seed=0).values()\n",
    "display(train.to_pandas())\n",
    "\n",
    "x_train = train['text']\n",
    "y_train = train['label']\n",
    "x_test = test['text']\n",
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425edca6-9479-4878-b34c-32faaf30bc18",
   "metadata": {},
   "source": [
    "## Building and Training the Model\n",
    "\n",
    "Alright, now that we're done with the data, it's time to build the classification pipeline. The first step would be **vectorization** - the process of turning strings to manipulate them mathematically. The easiest way to do so is to use a **count vectorizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9438a3e-158e-414c-afa0-0daf65559b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdbda53-4f6a-4e38-8d32-90ea8904343e",
   "metadata": {},
   "source": [
    "The idea is pretty simple. \n",
    "\n",
    "First, it scans all the text to build a vocabulary of all the unique words it finds. Then, for each sentence, it creates a numerical list (vector) where each number describes how many times a specific word from that dictionary appears in that sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8640097-90a4-4903-a8b9-64fcf75095cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bye', 'hello', 'world'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 1],\n",
       "       [1, 0, 1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_vector = vectorizer.fit_transform(['Hello World! Hello!', 'Bye World']).toarray()\n",
    "display(vectorizer.get_feature_names_out(), example_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe7f9c8-9bae-4dbc-812f-9427c347aa27",
   "metadata": {},
   "source": [
    "Sounds easy, right? Now, we may define a thing called **scaler**.\n",
    "\n",
    "It's mostly optional, but it could help us rescale numerical features (like word counts) so they have a similar range. It could potentially help the model to learn better by preventing features with naturally larger values from unfairly dominating the learning process, ensuring all features contribute more equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f88c46dd-565a-49bc-9468-c14c35d9e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_mean=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa51544-0831-4c43-b150-73d5a8c31770",
   "metadata": {},
   "source": [
    "Finally, let's define a classifier (no fancy configuration here *yet*) and stick everything into an elegant pipeline. That is going to be our final **model architecture**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4687d9be-8a20-42fa-b0bb-d2ee4be3ed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('scaler', scaler),\n",
    "    ('classifier', classifier),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc5131a-3090-4fc5-9439-b09d72563eb8",
   "metadata": {},
   "source": [
    "We could start training our model right away, but...\n",
    "\n",
    "It would be not ideal in terms of its *hyperparameters* - those values that define *how* our pipeline works. These are settings we choose before training, like regularization strength or how our text vectorizer processes words. They significantly control how the model learns and how well it ultimately performs.\n",
    "\n",
    "Manually trying every possible combination of these hyperparameters would be incredibly tedious. Instead, we can use automated hyperparameter tuning techniques. One such technique is **randomized search** - it randomly samples different combinations of hyperparameters from a pre-defined grid.\n",
    "\n",
    "We may tune the following parameters:\n",
    "\n",
    "- **Classifier `C`**: Regularization strength of the LogisticRegression classifier. Smaller values make it stronger (less prone to overfitting), and bigger - weaker (able to capture more nuances in noisy data).\n",
    "- **Vectorizer `ngram_range`**: This is crucial for capturing context! Instead of just looking at individual words (unigrams), n-grams allow us to consider sequences of words as single features. Using n-grams beyond unigrams often significantly improves performance in text tasks by providing more contextual information to the model, but it also increases the vocabulary size.\n",
    "- **Vectorizer `max_df`**: Maximum document frequency - ignore terms that appear in more than 'max_df' documents. Smaller values exclude more common terms (good for noise reduction), but too small may result in losing important common signals (underfitting).\n",
    "- **Vectorizer `min_df`**: Minimum document frequency - ignore terms that appear in fewer than 'min_df' documents. Smaller values may lead to huge noisy vocabularies, and bigger ones may result in losing specific signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db445460-7103-4a26-a64a-e2404f49a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2)], \n",
    "    'vectorizer__max_df': [0.85, 0.90, 0.95, 1.0],\n",
    "    'vectorizer__min_df': [1, 2, 3, 5],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfafe946-117a-494b-8bfd-74e15a6a1604",
   "metadata": {},
   "source": [
    "Everything is ready - let's train our model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11105c57-1e26-4f55-a16a-34fabf12785e",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 2/5] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.874 total time=   5.7s\n",
      "[CV 1/5] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.869 total time=   5.7s\n",
      "[CV 3/5] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.873 total time=   5.8s\n",
      "[CV 4/5] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.871 total time=   5.7s\n",
      "[CV 1/5] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1);, score=0.872 total time=   5.5s\n",
      "[CV 5/5] END classifier__C=0.1, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.871 total time=   5.7s\n",
      "[CV 2/5] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1);, score=0.877 total time=   5.5s\n",
      "[CV 3/5] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1);, score=0.876 total time=   5.7s\n",
      "[CV 4/5] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1);, score=0.866 total time=   6.4s\n",
      "[CV 5/5] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1);, score=0.867 total time=   6.4s\n",
      "[CV 1/5] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.870 total time=   6.5s\n",
      "[CV 2/5] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.876 total time=   6.5s\n",
      "[CV 3/5] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.874 total time=   6.6s\n",
      "[CV 4/5] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.872 total time=   6.7s\n",
      "[CV 5/5] END classifier__C=0.1, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.872 total time=   6.1s\n",
      "[CV 1/5] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1);, score=0.874 total time=   6.0s\n",
      "[CV 2/5] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1);, score=0.875 total time=   6.1s\n",
      "[CV 3/5] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1);, score=0.876 total time=   6.3s\n",
      "[CV 1/5] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.901 total time=  19.0s\n",
      "[CV 4/5] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1);, score=0.874 total time=   8.1s\n",
      "[CV 5/5] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=3, vectorizer__ngram_range=(1, 1);, score=0.870 total time=   8.2s\n",
      "[CV 2/5] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.907 total time=  22.7s\n",
      "[CV 3/5] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.906 total time=  22.6s\n",
      "[CV 5/5] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.900 total time=  22.7s\n",
      "[CV 4/5] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.904 total time=  22.9s\n",
      "[CV 1/5] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1);, score=0.873 total time=   6.9s\n",
      "[CV 2/5] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1);, score=0.878 total time=   7.1s\n",
      "[CV 3/5] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1);, score=0.876 total time=   7.0s\n",
      "[CV 4/5] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1);, score=0.870 total time=   7.7s\n",
      "[CV 1/5] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.868 total time=   7.5s\n",
      "[CV 5/5] END classifier__C=0.1, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1);, score=0.873 total time=   8.2s\n",
      "[CV 1/5] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2);, score=0.897 total time=  32.0s\n",
      "[CV 2/5] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.869 total time=   7.7s\n",
      "[CV 2/5] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2);, score=0.906 total time=  32.3s\n",
      "[CV 3/5] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.872 total time=   7.9s\n",
      "[CV 4/5] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.870 total time=   7.7s\n",
      "[CV 3/5] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2);, score=0.905 total time=  32.7s\n",
      "[CV 5/5] END classifier__C=10, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.870 total time=   8.3s\n",
      "[CV 4/5] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2);, score=0.897 total time=  33.5s\n",
      "[CV 1/5] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.874 total time=   6.6s\n",
      "[CV 5/5] END classifier__C=10, vectorizer__max_df=0.95, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2);, score=0.896 total time=  30.8s\n",
      "[CV 2/5] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.876 total time=   6.4s\n",
      "[CV 3/5] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.878 total time=   6.5s\n",
      "[CV 4/5] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.873 total time=   5.8s\n",
      "[CV 5/5] END classifier__C=1, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.872 total time=   5.8s\n",
      "[CV 1/5] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.868 total time=   5.8s\n",
      "[CV 2/5] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.877 total time=   5.6s\n",
      "[CV 3/5] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.873 total time=   4.2s\n",
      "[CV 4/5] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.874 total time=   4.0s\n",
      "[CV 5/5] END classifier__C=10, vectorizer__max_df=0.85, vectorizer__min_df=2, vectorizer__ngram_range=(1, 1);, score=0.863 total time=   3.9s\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "from joblib import parallel_backend\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "cv = RandomizedSearchCV(pipeline, param_grid, random_state=0, n_jobs=-1, verbose=3)\n",
    "cv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861cf7c-2907-4ad4-ae8c-bc41d3899e5d",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86b04d1e-f56a-4e76-8b88-8f116e1ba5dd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.91      0.90      0.90      5025\n",
      "         pos       0.90      0.91      0.90      4975\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "prediction = cv.best_estimator_.predict(x_test)\n",
    "print(classification_report(y_test, prediction, target_names=ds.features['label'].names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "286658e5-e369-49e2-9d09-2c2517ef50d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_caaa4\">\n",
       "  <thead>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_caaa4_row0_col0\" class=\"data row0 col0\" >vectorizer__ngram_range</td>\n",
       "      <td id=\"T_caaa4_row0_col1\" class=\"data row0 col1\" >(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_caaa4_row1_col0\" class=\"data row1 col0\" >vectorizer__min_df</td>\n",
       "      <td id=\"T_caaa4_row1_col1\" class=\"data row1 col1\" >5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_caaa4_row2_col0\" class=\"data row2 col0\" >vectorizer__max_df</td>\n",
       "      <td id=\"T_caaa4_row2_col1\" class=\"data row2 col1\" >0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_caaa4_row3_col0\" class=\"data row3 col0\" >classifier__C</td>\n",
       "      <td id=\"T_caaa4_row3_col1\" class=\"data row3 col1\" >1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x178ac6d40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "best_params_table = DataFrame.from_dict(cv.best_params_.items())\n",
    "display(best_params_table.style.hide(axis=0).hide(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf35528-810d-47ca-805a-73886014c47e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Our logistic regression model reached an accuracy of **90%**. \n",
    "\n",
    "This demonstrates the effectiveness of classical machine learning techniques for the type of text classification task. One of the key factors contributing to this performance was the use of n-grams for local context processing.\n",
    "\n",
    "While being highly effective, future improvements could involve exploring more complex vectorization techniques, experimenting with more advanced text pre-processing (like stemming or lemmatization), or even moving to deep learning models for sequence understanding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
