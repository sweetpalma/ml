{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1328c26f-f4c4-459e-b0e1-51208b368377",
   "metadata": {},
   "source": [
    "# Sentiment Analysis (LSTM and Word2Vec)\n",
    "\n",
    "In our [previous MLP experiment](./03-mlp.ipynb), we saw that a neural network could extract more from averaged Word2Vec embeddings than a simpler Logistic Regression. However, averaging throws away crucial information: the *order of words* in a sentence.\n",
    "\n",
    "This time, we're diving deeper into neural networks with **Long Short-Term Memory (LSTM)** units. LSTMs are specifically designed to process sequences, remembering context and understanding how word order contributes to meaning. Let's see if harnessing this sequential power can push our accuracy even further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7778f81-c1de-4370-8059-7746a7f51ff3",
   "metadata": {},
   "source": [
    "## Data Preparapation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebf07db0-756f-4ff1-bc78-e47f125eb564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONUNBUFFERED=1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74676</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61691 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                               text\n",
       "0      Positive  I am coming to the borders and I will kill you...\n",
       "1      Positive  im getting on borderlands and i will kill you ...\n",
       "2      Positive  im coming on borderlands and i will murder you...\n",
       "3      Positive  im getting on borderlands 2 and i will murder ...\n",
       "4      Positive  im getting into borderlands and i can murder y...\n",
       "...         ...                                                ...\n",
       "74676  Positive  Just realized that the Windows partition of my...\n",
       "74677  Positive  Just realized that my Mac window partition is ...\n",
       "74678  Positive  Just realized the windows partition of my Mac ...\n",
       "74679  Positive  Just realized between the windows partition of...\n",
       "74680  Positive  Just like the windows partition of my Mac is l...\n",
       "\n",
       "[61691 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%env PYTHONUNBUFFERED=1\n",
    "import kagglehub\n",
    "df = kagglehub.dataset_load(\n",
    "    kagglehub.KaggleDatasetAdapter.PANDAS,\n",
    "    'jp797498e/twitter-entity-sentiment-analysis',\n",
    "    'twitter_training.csv',\n",
    "    pandas_kwargs={'encoding': 'ISO-8859-1'},\n",
    ")\n",
    "\n",
    "df = df[df.columns[[2, 3]]]\n",
    "df.columns = ['sentiment', 'text']\n",
    "\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['sentiment'] = df['sentiment'].astype(str)\n",
    "df = df.dropna()\n",
    "\n",
    "df = df.loc[df['sentiment'] != 'Irrelevant']\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "abad53d3-c693-4d77-b119-e5b9bf032dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "x_train = train['text']\n",
    "y_train = train['sentiment']\n",
    "x_test = test['text']\n",
    "y_test = test['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b4cbf-1492-4737-9c70-898e86d4bb7f",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d11f5396-977f-4fd6-aa97-623b8cb3a5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(df['sentiment'])\n",
    "\n",
    "y_train_encoded = pd.DataFrame(encoder.fit_transform(y_train))\n",
    "y_test_encoded = pd.DataFrame(encoder.transform(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554972f5-640d-45a8-b5b6-f12d663ea3b9",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Before our neural network can understand text, we need to convert sentences into a numerical format it can process. In our MLP approach, we tokenized text and then immediately averaged word vectors. LSTM pipeline is a *a bit more complicated*.\n",
    "\n",
    "The first step is *tokenization*, where we break down each sentence into individual words or sub-word units called \"tokens.\" Then, we'll build a vocabulary of all unique tokens in our training data and assign a unique integer ID to each one, transforming our text into sequences of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "718fb948-3b18-4c9f-acf1-fd0cb0c31c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[737, 162]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "display(tokenizer.texts_to_sequences([\"Hello World\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b724b6-e573-483b-ac63-ba0adc323d24",
   "metadata": {},
   "source": [
    "## Data Padding\n",
    "\n",
    "The next step is called padding. LSTMs (like most neural networks) require input sequences to be of a uniform length. Since our sentences naturally vary, we need to **pad** our integer sequences to a **fixed length**.\n",
    "\n",
    "This involves either truncating longer sequences or adding special tokens (usually zeros) to shorter sequences until they all reach a predetermined maximum length. We can use some *reasonable* number here, and adjust it later if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a705e757-43a8-475b-bdd6-8636769f3f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,   365,    10,     7],\n",
       "       [    0,     0,     0, ...,  3400,   185,   614],\n",
       "       [    0,     0,     0, ...,  1038,   555, 17964],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,   267,   910,  1756],\n",
       "       [    0,     0,     0, ...,     9,    27,    62],\n",
       "       [    0,     0,     0, ...,    16,   113,   841]], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maxlen = 96\n",
    "\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "x_train = pad_sequences(tokenizer.texts_to_sequences(x_train), maxlen=maxlen)\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(x_test), maxlen=maxlen)\n",
    "\n",
    "display(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f1e4da-04dd-493c-bb4d-1b89ee6dc07d",
   "metadata": {},
   "source": [
    "## Embedding Layer\n",
    "\n",
    "Now we need to transform these padded sequences into meaningful representations that capture their semantic relationships. This is where the **embedding** layer comes in.\n",
    "\n",
    "Think of it as a sophisticated, trainable lookup table - for each integer ID representing a token in our sequence, the embedding layer looks up its corresponding dense vector. This helps our neural network to capture not some random indexes, but the *semantic* meaning of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e222766-81f8-4bfa-b810-17673d27191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_shape = (len(tokenizer.word_index) + 1, wv.vector_size)\n",
    "embedding_matrix = np.zeros(shape=embedding_matrix_shape)\n",
    "for word, index in tokenizer.word_index.items():\n",
    "  if word in wv:\n",
    "    embedding_matrix[index] = wv.get_vector(word)\n",
    "  else:\n",
    "    embedding_matrix[index] = np.zeros(wv.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a1dbbc-fa62-4355-af58-fe4f63a030f1",
   "metadata": {},
   "source": [
    "## Building and Training the Model\n",
    "\n",
    "Here comes the most interesting part. With our text now represented as sequences of dense semantic vectors (thanks to the embedding layer), we can introduce the star of this experiment - the **Long Short-Term Memory (LSTM)** layer.\n",
    "\n",
    "Unlike a simple Dense layer that processes all its inputs at once, an LSTM processes each vector in our sequence one at a time. Internally, each LSTM unit contains a sophisticated set of \"gates\" – an input gate, a forget gate, and an output gate – along with a memory cell.\n",
    "\n",
    "These gates learn to control the flow of information, deciding what to remember from previous steps, what to discard, and what new information from the current word's vector is important enough to update its memory, allowing it to capture *context* and *dependencies* across the entire sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8333ee3c-872d-4ac2-9ef6-3f0c56a40826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Sequential\n",
    "num_classes = len(encoder.classes_)\n",
    "model = Sequential([\n",
    "  layers.Embedding(\n",
    "    weights=[embedding_matrix],\n",
    "    input_dim=embedding_matrix_shape[0], \n",
    "    output_dim=embedding_matrix_shape[1],\n",
    "    trainable=True, # allow optimizer to tweak this layer too\n",
    "    mask_zero=True, # ignore padding zeroes\n",
    "  ),\n",
    "  layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(num_classes, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32172465-7b6f-4a1a-9f1b-103f139f643b",
   "metadata": {},
   "source": [
    "We can compile and train our model now, but before we do so - let's implement a regularisation technique called **early stopping**. Essentially, that's a special function that watches our training process and stops it when the monitored metric stops improving. That helps to reduce overfitting and saves us some computation cycles when the training process gets stuck with the same accuracy for too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b8647282-d4c3-4ee2-ab69-479e35677ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "earlystop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0b191b-5984-416f-8409-cb993f800703",
   "metadata": {},
   "source": [
    "Also, we could try tweaking the optimizer learning rate - the default one might be too high when fine-tuning pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3be945fe-c351-4a71-899e-20bd0ed86905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab0c0f-dc5a-41f0-ba5f-d6200cbd8144",
   "metadata": {},
   "source": [
    "Everything is ready - let's start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4844a91a-c0f1-454b-961d-ec0d16eaf829",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 281ms/step - accuracy: 0.9663 - loss: 0.0700 - val_accuracy: 0.9127 - val_loss: 0.3001\n",
      "Epoch 2/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 294ms/step - accuracy: 0.9693 - loss: 0.0654 - val_accuracy: 0.9127 - val_loss: 0.3082\n",
      "Epoch 3/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 288ms/step - accuracy: 0.9691 - loss: 0.0642 - val_accuracy: 0.9137 - val_loss: 0.3113\n",
      "Epoch 4/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 279ms/step - accuracy: 0.9692 - loss: 0.0641 - val_accuracy: 0.9135 - val_loss: 0.3162\n",
      "Epoch 5/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 268ms/step - accuracy: 0.9702 - loss: 0.0624 - val_accuracy: 0.9131 - val_loss: 0.3196\n",
      "Epoch 6/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 282ms/step - accuracy: 0.9705 - loss: 0.0606 - val_accuracy: 0.9125 - val_loss: 0.3341\n",
      "Epoch 7/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 278ms/step - accuracy: 0.9704 - loss: 0.0610 - val_accuracy: 0.9119 - val_loss: 0.3264\n",
      "Epoch 8/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 271ms/step - accuracy: 0.9706 - loss: 0.0594 - val_accuracy: 0.9133 - val_loss: 0.3412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x416539990>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train_encoded, epochs=35, batch_size=128, validation_split=0.1, callbacks=[earlystop]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff4ab5f-7b61-4d05-9326-a6c22ae5083a",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "47ff80d1-d975-4934-877f-610e21440a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.93      0.91      0.92      4553\n",
      "     Neutral       0.90      0.91      0.90      3670\n",
      "    Positive       0.91      0.91      0.91      4116\n",
      "\n",
      "    accuracy                           0.91     12339\n",
      "   macro avg       0.91      0.91      0.91     12339\n",
      "weighted avg       0.91      0.91      0.91     12339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred_probs = model.predict(x_test, verbose=False)\n",
    "y_pred_labels = np.argmax(y_pred_probs, axis=1)\n",
    "y_true_labels = np.argmax(y_test_encoded.to_numpy(), axis=1)\n",
    "print(classification_report(y_true_labels, y_pred_labels, target_names=encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda7276-fb61-44a1-a38b-cbf544ba5f1a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "By leveraging Long Short-Term Memory (LSTM) units and fine-tuning pre-trained Word2Vec embeddings, this model successfully harnessed sequential information, achieving a final accuracy of 91%.\n",
    "\n",
    "This proves the value of sequence-aware models over simple embedding averaging for this dataset, performing competitively with the heavily optimized CountVectorizer approach. Further improvements would likely require exploring more advanced architectures like [transformers](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "But for now... This is more than enough for this class of tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
