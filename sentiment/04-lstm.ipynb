{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1328c26f-f4c4-459e-b0e1-51208b368377",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory\n",
    "\n",
    "In our [previous MLP experiment](./03-mlp.ipynb), we saw that a neural network could extract more from averaged Word2Vec embeddings than a simpler Logistic Regression. However, averaging throws away crucial information: the *order of words* in a sentence.\n",
    "\n",
    "This time, we're diving deeper into neural networks with **Long Short-Term Memory (LSTM)** units. They are specifically designed to process sequences, remembering context and understanding how word order contributes to meaning. Let's see if harnessing this sequential power can push our accuracy even further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7778f81-c1de-4370-8059-7746a7f51ff3",
   "metadata": {},
   "source": [
    "## Data Preparapation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebf07db0-756f-4ff1-bc78-e47f125eb564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74676</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61691 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                               text\n",
       "0      Positive  I am coming to the borders and I will kill you...\n",
       "1      Positive  im getting on borderlands and i will kill you ...\n",
       "2      Positive  im coming on borderlands and i will murder you...\n",
       "3      Positive  im getting on borderlands 2 and i will murder ...\n",
       "4      Positive  im getting into borderlands and i can murder y...\n",
       "...         ...                                                ...\n",
       "74676  Positive  Just realized that the Windows partition of my...\n",
       "74677  Positive  Just realized that my Mac window partition is ...\n",
       "74678  Positive  Just realized the windows partition of my Mac ...\n",
       "74679  Positive  Just realized between the windows partition of...\n",
       "74680  Positive  Just like the windows partition of my Mac is l...\n",
       "\n",
       "[61691 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kagglehub\n",
    "df = kagglehub.dataset_load(\n",
    "    kagglehub.KaggleDatasetAdapter.PANDAS,\n",
    "    'jp797498e/twitter-entity-sentiment-analysis',\n",
    "    'twitter_training.csv',\n",
    "    pandas_kwargs={'encoding': 'ISO-8859-1'},\n",
    ")\n",
    "\n",
    "df = df[df.columns[[2, 3]]]\n",
    "df.columns = ['sentiment', 'text']\n",
    "\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['sentiment'] = df['sentiment'].astype(str)\n",
    "df = df.dropna()\n",
    "\n",
    "df = df.loc[df['sentiment'] != 'Irrelevant']\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abad53d3-c693-4d77-b119-e5b9bf032dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "x_train = train['text']\n",
    "y_train = train['sentiment']\n",
    "x_test = test['text']\n",
    "y_test = test['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554972f5-640d-45a8-b5b6-f12d663ea3b9",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Before our neural network can understand text, we need to convert sentences into a numerical format it can process. In our MLP approach, we tokenized text and then immediately averaged word vectors. LSTM pipeline is a *a bit more complicated*.\n",
    "\n",
    "The first step is *tokenization*, where we break down each sentence into individual words or sub-word units called \"tokens.\" Then, we'll build a vocabulary of all unique tokens in our training data and assign a unique integer ID to each one, transforming our text into sequences of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "718fb948-3b18-4c9f-acf1-fd0cb0c31c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[737, 162]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "display(tokenizer.texts_to_sequences([\"Hello World\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b724b6-e573-483b-ac63-ba0adc323d24",
   "metadata": {},
   "source": [
    "## Data Padding\n",
    "\n",
    "The next step is called padding. LSTMs (like most neural networks) require input sequences to be of a uniform length. Since our sentences naturally vary, we need to **pad** our integer sequences to a **fixed length**.\n",
    "\n",
    "This involves either truncating longer sequences or adding special tokens (usually zeros) to shorter sequences until they all reach a predetermined maximum length. We can use some *reasonable* number here, and adjust it later if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a705e757-43a8-475b-bdd6-8636769f3f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,   357,   155,   163],\n",
       "       [    0,     0,     0, ...,    25,  1136,    10],\n",
       "       [    0,     0,     0, ...,   827,  3072,  1119],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,   469,     7,  5669],\n",
       "       [    0,     0,     0, ...,    36,    16, 14165],\n",
       "       [    0,     0,     0, ...,     3,   121,    10]], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maxlen = 96\n",
    "\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "x_train = pad_sequences(tokenizer.texts_to_sequences(x_train), maxlen=maxlen)\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(x_test), maxlen=maxlen)\n",
    "\n",
    "display(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f1e4da-04dd-493c-bb4d-1b89ee6dc07d",
   "metadata": {},
   "source": [
    "## Embedding Layer\n",
    "\n",
    "Next, we need to transform these padded sequences into meaningful representations that capture their semantic relationships. That's where the **embedding** layer comes in. To build it, we may use our existing Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d87ef7df-f983-4f68-8fd4-ecbe96312d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\n",
    "    'leadbest/googlenewsvectorsnegative300', \n",
    "    path='GoogleNews-vectors-negative300.bin.gz'\n",
    ")\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "wv = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2883aa41-cc3c-4850-a70e-2630fde63690",
   "metadata": {},
   "source": [
    "Think of it as a sophisticated, trainable lookup table - for each integer ID representing a token in our sequence, the embedding layer looks up its corresponding dense vector. This helps our neural network to capture not some random indexes, but the *semantic* meaning of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1612d0e-fc73-4675-8534-de9f591129ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embedding_matrix_shape = (len(tokenizer.word_index) + 1, wv.vector_size)\n",
    "embedding_matrix = np.zeros(shape=embedding_matrix_shape)\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if word in wv:\n",
    "        embedding_matrix[index] = wv.get_vector(word)\n",
    "    else:\n",
    "        embedding_matrix[index] = np.zeros(wv.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638abb1-d4a5-4366-9731-1389be7692a3",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6c2f809-7c53-45b3-ae16-cc60ec1f12eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer \n",
    "import pandas as pd\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(df['sentiment'])\n",
    "\n",
    "y_train_encoded = pd.DataFrame(encoder.fit_transform(y_train))\n",
    "y_test_encoded = pd.DataFrame(encoder.transform(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a1dbbc-fa62-4355-af58-fe4f63a030f1",
   "metadata": {},
   "source": [
    "## Building and Training the Model\n",
    "\n",
    "Here comes the most interesting part. With our text now represented as sequences of dense semantic vectors (thanks to the embedding layer), we can introduce the star of this experiment - the **Long Short-Term Memory (LSTM)** layer.\n",
    "\n",
    "Unlike a simple Dense layer that processes all its inputs at once, an LSTM processes each vector in our sequence one at a time. Internally, each LSTM unit contains a sophisticated set of \"gates\" – an input gate, a forget gate, and an output gate – along with a memory cell.\n",
    "\n",
    "These gates learn to control the flow of information, deciding what to remember from previous steps, what to discard, and what new information from the current word's vector is important enough to update its memory, allowing it to capture *context* and *dependencies* across the entire sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8333ee3c-872d-4ac2-9ef6-3f0c56a40826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Sequential\n",
    "num_classes = len(encoder.classes_)\n",
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        weights=[embedding_matrix],\n",
    "        input_dim=embedding_matrix_shape[0], \n",
    "        output_dim=embedding_matrix_shape[1],\n",
    "        trainable=True, # allow optimizer to tweak this layer too\n",
    "        mask_zero=True, # ignore padding zeroes\n",
    "    ),\n",
    "    layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(num_classes, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32172465-7b6f-4a1a-9f1b-103f139f643b",
   "metadata": {},
   "source": [
    "We can compile and train our model now, but before we do so - let's implement a regularisation technique called **early stopping**. Essentially, that's a special function that watches our training process and stops it when the monitored metric stops improving. That helps to reduce overfitting and saves us some computation cycles when the training process gets stuck with the same accuracy for too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8647282-d4c3-4ee2-ab69-479e35677ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "earlystop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0b191b-5984-416f-8409-cb993f800703",
   "metadata": {},
   "source": [
    "Also, we could try tweaking the optimizer learning rate - the default one might be too high when fine-tuning pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3be945fe-c351-4a71-899e-20bd0ed86905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab0c0f-dc5a-41f0-ba5f-d6200cbd8144",
   "metadata": {},
   "source": [
    "Everything is ready - let's start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4844a91a-c0f1-454b-961d-ec0d16eaf829",
   "metadata": {
    "scrolled": true,
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 280ms/step - accuracy: 0.4975 - loss: 1.0252 - val_accuracy: 0.6669 - val_loss: 0.8161\n",
      "Epoch 2/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 275ms/step - accuracy: 0.6696 - loss: 0.8008 - val_accuracy: 0.7111 - val_loss: 0.7120\n",
      "Epoch 3/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 272ms/step - accuracy: 0.7129 - loss: 0.7021 - val_accuracy: 0.7466 - val_loss: 0.6414\n",
      "Epoch 4/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 281ms/step - accuracy: 0.7468 - loss: 0.6350 - val_accuracy: 0.7747 - val_loss: 0.5757\n",
      "Epoch 5/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 279ms/step - accuracy: 0.7780 - loss: 0.5642 - val_accuracy: 0.7934 - val_loss: 0.5168\n",
      "Epoch 6/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 294ms/step - accuracy: 0.8068 - loss: 0.4949 - val_accuracy: 0.8144 - val_loss: 0.4728\n",
      "Epoch 7/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 289ms/step - accuracy: 0.8251 - loss: 0.4517 - val_accuracy: 0.8306 - val_loss: 0.4352\n",
      "Epoch 8/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 288ms/step - accuracy: 0.8432 - loss: 0.4051 - val_accuracy: 0.8424 - val_loss: 0.4055\n",
      "Epoch 9/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 296ms/step - accuracy: 0.8585 - loss: 0.3652 - val_accuracy: 0.8515 - val_loss: 0.3836\n",
      "Epoch 10/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 275ms/step - accuracy: 0.8708 - loss: 0.3330 - val_accuracy: 0.8614 - val_loss: 0.3566\n",
      "Epoch 11/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 285ms/step - accuracy: 0.8769 - loss: 0.3160 - val_accuracy: 0.8695 - val_loss: 0.3366\n",
      "Epoch 12/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 279ms/step - accuracy: 0.8879 - loss: 0.2840 - val_accuracy: 0.8736 - val_loss: 0.3243\n",
      "Epoch 13/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 277ms/step - accuracy: 0.8954 - loss: 0.2648 - val_accuracy: 0.8801 - val_loss: 0.3098\n",
      "Epoch 14/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 285ms/step - accuracy: 0.9040 - loss: 0.2467 - val_accuracy: 0.8819 - val_loss: 0.2973\n",
      "Epoch 15/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 277ms/step - accuracy: 0.9057 - loss: 0.2360 - val_accuracy: 0.8872 - val_loss: 0.2875\n",
      "Epoch 16/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 278ms/step - accuracy: 0.9116 - loss: 0.2204 - val_accuracy: 0.8882 - val_loss: 0.2793\n",
      "Epoch 17/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 277ms/step - accuracy: 0.9164 - loss: 0.2075 - val_accuracy: 0.8863 - val_loss: 0.2786\n",
      "Epoch 18/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 275ms/step - accuracy: 0.9193 - loss: 0.1988 - val_accuracy: 0.8928 - val_loss: 0.2685\n",
      "Epoch 19/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 285ms/step - accuracy: 0.9260 - loss: 0.1858 - val_accuracy: 0.8951 - val_loss: 0.2658\n",
      "Epoch 20/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 285ms/step - accuracy: 0.9264 - loss: 0.1805 - val_accuracy: 0.8997 - val_loss: 0.2587\n",
      "Epoch 21/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 286ms/step - accuracy: 0.9298 - loss: 0.1721 - val_accuracy: 0.8967 - val_loss: 0.2626\n",
      "Epoch 22/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 283ms/step - accuracy: 0.9327 - loss: 0.1649 - val_accuracy: 0.8997 - val_loss: 0.2547\n",
      "Epoch 23/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 281ms/step - accuracy: 0.9375 - loss: 0.1566 - val_accuracy: 0.9034 - val_loss: 0.2527\n",
      "Epoch 24/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 291ms/step - accuracy: 0.9369 - loss: 0.1530 - val_accuracy: 0.9001 - val_loss: 0.2558\n",
      "Epoch 25/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 287ms/step - accuracy: 0.9400 - loss: 0.1461 - val_accuracy: 0.9030 - val_loss: 0.2515\n",
      "Epoch 26/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 293ms/step - accuracy: 0.9453 - loss: 0.1354 - val_accuracy: 0.9034 - val_loss: 0.2505\n",
      "Epoch 27/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 283ms/step - accuracy: 0.9426 - loss: 0.1358 - val_accuracy: 0.9052 - val_loss: 0.2506\n",
      "Epoch 28/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 282ms/step - accuracy: 0.9433 - loss: 0.1349 - val_accuracy: 0.9094 - val_loss: 0.2502\n",
      "Epoch 29/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 267ms/step - accuracy: 0.9463 - loss: 0.1289 - val_accuracy: 0.9090 - val_loss: 0.2476\n",
      "Epoch 30/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 270ms/step - accuracy: 0.9466 - loss: 0.1253 - val_accuracy: 0.9074 - val_loss: 0.2496\n",
      "Epoch 31/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 277ms/step - accuracy: 0.9491 - loss: 0.1197 - val_accuracy: 0.9066 - val_loss: 0.2509\n",
      "Epoch 32/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 283ms/step - accuracy: 0.9494 - loss: 0.1176 - val_accuracy: 0.9094 - val_loss: 0.2500\n",
      "Epoch 33/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 283ms/step - accuracy: 0.9511 - loss: 0.1133 - val_accuracy: 0.9098 - val_loss: 0.2493\n",
      "Epoch 34/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 286ms/step - accuracy: 0.9519 - loss: 0.1120 - val_accuracy: 0.9094 - val_loss: 0.2564\n",
      "Epoch 35/35\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 293ms/step - accuracy: 0.9506 - loss: 0.1130 - val_accuracy: 0.9119 - val_loss: 0.2541\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import device\n",
    "with device('/CPU:0'):\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train_encoded, epochs=35, batch_size=128, validation_split=0.1, callbacks=[earlystop]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff4ab5f-7b61-4d05-9326-a6c22ae5083a",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47ff80d1-d975-4934-877f-610e21440a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.92      0.93      0.92      4605\n",
      "     Neutral       0.89      0.91      0.90      3594\n",
      "    Positive       0.92      0.89      0.91      4140\n",
      "\n",
      "    accuracy                           0.91     12339\n",
      "   macro avg       0.91      0.91      0.91     12339\n",
      "weighted avg       0.91      0.91      0.91     12339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "with device('/CPU:0'):\n",
    "    y_pred_probs = model.predict(x_test, verbose=False)\n",
    "    y_pred_labels = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true_labels = np.argmax(y_test_encoded.to_numpy(), axis=1)\n",
    "    print(classification_report(y_true_labels, y_pred_labels, target_names=encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda7276-fb61-44a1-a38b-cbf544ba5f1a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We were able to achieve a final accuracy of **91%** - this was accomplished by combining LSTM units with pre-trained Word2Vec embeddings, enabling the model to leverage sequential information.\n",
    "\n",
    "This proves the value of sequence-aware models over simple embedding averaging for this dataset, performing competitively with the heavily optimized CountVectorizer approach. Further improvements would likely require exploring more advanced architectures like [transformers](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "But for now... This is more than enough for this class of tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
