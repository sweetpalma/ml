{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46d3944c-487f-4abe-8b2b-27d31e638086",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "In our [previous notebook](./01-logistic-regression.ipynb), we built a solid sentiment classifier using logistic regression and simple count vectorizer. While that model performed well by focusing on word counts and phrases, it treated each word largely as an independent entity without understanding its underlying meaning or relationship to other words.\n",
    "\n",
    "This time, we'll delve into **word embeddings** – a technique that represents words as dense vectors, capturing their semantic relationships and context within a large body of text. Our today's goal is to integrate them into our logistic regression pipeline and see how they will affect our sentiment analysis score.\n",
    "\n",
    "<!-- Articles used:\n",
    "- https://medium.com/swlh/sentiment-classification-using-word-embeddings-word2vec-aedf28fbb8ca\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317ef3f5-07f5-4848-8fa3-930e4f19d414",
   "metadata": {},
   "source": [
    "##  Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d659da78-4231-4267-bd1f-161f6b24d15d",
   "metadata": {},
   "source": [
    "We are going to use the same dataset as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "756edb3b-8ae4-4256-8c75-2000ab7ffca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After the SuperFriends and Scooby Doo left the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good job.that's how i would describe this anim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michael Cacoyannis has had a relatively long c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've just seen this film in a lovely air-condi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My one-line summary hints that this is not a g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>***SPOILERS*** ***SPOILERS*** After two so-so ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>Way back in 1967, a certain director had no id...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>I saw this movie with my dad. I must have been...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>During my teens or should I say prime time I w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>Paranoid Park is about Alex, a 16 year old ska...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      After the SuperFriends and Scooby Doo left the...      1\n",
       "1      good job.that's how i would describe this anim...      1\n",
       "2      Michael Cacoyannis has had a relatively long c...      1\n",
       "3      I've just seen this film in a lovely air-condi...      0\n",
       "4      My one-line summary hints that this is not a g...      1\n",
       "...                                                  ...    ...\n",
       "39995  ***SPOILERS*** ***SPOILERS*** After two so-so ...      1\n",
       "39996  Way back in 1967, a certain director had no id...      0\n",
       "39997  I saw this movie with my dad. I must have been...      1\n",
       "39998  During my teens or should I say prime time I w...      1\n",
       "39999  Paranoid Park is about Alex, a 16 year old ska...      0\n",
       "\n",
       "[40000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('stanfordnlp/imdb', split='train+test')\n",
    "train, test = ds.train_test_split(test_size=0.2, seed=0).values()\n",
    "display(train.to_pandas())\n",
    "\n",
    "x_train = train['text']\n",
    "y_train = train['label']\n",
    "x_test = test['text']\n",
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120002dc-ea5f-4b76-81cb-60455283e2d2",
   "metadata": {},
   "source": [
    "This time, we will do some data preprocessing. For each text sample, we are going to apply a technique called **semantic vectorization**. Its core idea is to vectorize separate words into a thing called **word embedding** instead of a simple index.\n",
    "\n",
    "What is a word embedding? Essentially, it is a mathematical representation of a word (or phrase) as a vector (a numerical array) in a high-dimensional space. These vectors capture the semantic meaning of the word by representing its relationships to other words in a corpus of text. That means that semantically similar words would be close in this high-dimensional space. \n",
    "\n",
    "This approach requires a separate vectorization model - we could start with a pre-trained model called GoogleNews300. It contains 300-dimensional vectors for 3 million words and phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ced9d1-f1c3-437f-b9ce-469829ab6701",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5023afde8ac485e98ced4310c37d730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from os import path\n",
    "from huggingface_hub import snapshot_download\n",
    "model_path = path.join(snapshot_download('fse/word2vec-google-news-300'), 'word2vec-google-news-300.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bfb7e6-0c5d-4e5c-aeb5-8d9bdaa2e50d",
   "metadata": {},
   "source": [
    "Let's see it in action by loading it and comparing the similarities of different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "539c762d-c976-4324-825d-0867db36a369",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg' (/opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.12/site-packages/scipy/linalg/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors\n\u001b[32m      2\u001b[39m wv = KeyedVectors.load(model_path)\n\u001b[32m      4\u001b[39m pairs = [\n\u001b[32m      5\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mcar\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mminivan\u001b[39m\u001b[33m'\u001b[39m),   \u001b[38;5;66;03m# a minivan is a kind of car\u001b[39;00m\n\u001b[32m      6\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mcar\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mbicycle\u001b[39m\u001b[33m'\u001b[39m),   \u001b[38;5;66;03m# still a wheeled vehicle\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mcar\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcommunism\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     10\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.12/site-packages/gensim/__init__.py:11\u001b[39m\n\u001b[32m      7\u001b[39m __version__ = \u001b[33m'\u001b[39m\u001b[33m4.3.2\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m     14\u001b[39m logger = logging.getLogger(\u001b[33m'\u001b[39m\u001b[33mgensim\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger.handlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.12/site-packages/gensim/corpora/__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexedcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmmcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbleicorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.12/site-packages/gensim/corpora/indexedcorpus.py:14\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[32m     16\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIndexedCorpus\u001b[39;00m(interfaces.CorpusABC):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.12/site-packages/gensim/interfaces.py:19\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[33;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[32m     22\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCorpusABC\u001b[39;00m(utils.SaveLoad):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.12/site-packages/gensim/matutils.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs, triu\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlapack\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspecial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'triu' from 'scipy.linalg' (/opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.12/site-packages/scipy/linalg/__init__.py)"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "wv = KeyedVectors.load(model_path)\n",
    "\n",
    "pairs = [\n",
    "    ('car', 'minivan'),   # a minivan is a kind of car\n",
    "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
    "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
    "    ('car', 'cereal'),    # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b8c26-6b5c-4eec-89f3-3e15f253d122",
   "metadata": {},
   "source": [
    "That makes some sense, right? Communism has poor relations with cars.\\\n",
    "But what do those vectors look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f2a8d3-4068-4e8e-a11e-ce5e5ee00b56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(wv.get_vector('hello'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87d1278-df91-41d2-9169-a6166b9d7d3e",
   "metadata": {},
   "source": [
    "Now we need to build a vectorization routine. For each sequence we will perform a simple tokenization, extract embeddings, and then squash them into a single **averaged** vector. \n",
    "\n",
    "But why?\n",
    "\n",
    "The reason is simple - most traditional classifiers (like logistic regression) are fundamentally designed to work with fixed-size, flat feature vectors. They don't have an inherent mechanism to understand or process sequences of varying lengths or the temporal relationships within those sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f09ac5a-0f7c-46e4-a7a3-1c1af8c7f9d9",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np\n",
    "\n",
    "def vectorize(text):\n",
    "    tokens = simple_preprocess(text.lower(), deacc=True)\n",
    "    token_vectors = [wv.get_vector(x) for x in tokens if x in wv]\n",
    "    if token_vectors:\n",
    "        return np.mean(token_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(wv.vector_size)\n",
    "\n",
    "display(vectorize('Hello World!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e1d6b-acfa-4e7e-9697-7ecfb9a97d80",
   "metadata": {},
   "source": [
    "## Building and Training the Model\n",
    "\n",
    "Our final pipeline will remain almost the same - vectorizer, followed by a logistic regression classifier. Defining a simple cross-validation grid is a nice idea as well - but it will contain only one parameter now (regularisation strength)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c742b2-f8ea-4d0b-a0ae-08dcc11f9a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vectorizer = FunctionTransformer(lambda x: np.vstack([vectorize(seq) for seq in x]))\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d04cfce-6c37-46a6-a536-bc184c5e6412",
   "metadata": {},
   "source": [
    "The model is ready to be trained. This time, we may use **grid search** - our parameters matrix is so tiny that we could afford a full hyperplanar parameter search instead of a randomized one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e868068-154c-4c1e-8191-4b761ff06ec7",
   "metadata": {
    "scrolled": true,
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "from joblib import parallel_backend\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "cv = GridSearchCV(pipeline, param_grid, n_jobs=-1, verbose=3)\n",
    "cv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2539352f-da8a-46c4-9d35-1dac7c5f5665",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd702150-5f1d-45f2-99f5-fbdf6d88fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "prediction = cv.best_estimator_.predict(x_test)\n",
    "print(classification_report(y_test, prediction, target_names=ds.features['label'].names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ecdace-f6db-4b56-ab9c-f3784cf33186",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The model achieved a final accuracy of **86%**. While this demonstrates the basic application of word embeddings, it significantly underperforms compared to the previous approach using a simple count vectorizer with n-grams (which reached 90%).\n",
    "\n",
    "This suggests that for this specific dataset and task, the simple averaging of word embeddings, which loses word order and contextual nuances, is less effective than a feature representation that explicitly captures local phrases (like n-grams).\n",
    "\n",
    "This highlights a limitation of simple averaging - it discards crucial word order and local contextual information, which n-grams successfully captured. To overcome this and fully leverage the semantic power of word embeddings without losing sequential context, we need models that can learn to understand the relationships between words in a sentence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
